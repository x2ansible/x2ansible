# x2ansible - convert chef/puppet to ansible playbooks using LlamaStack Agents (RAG-powered)

Welcome to **x2ansible** â€”  
an AI-powered tool to **analyze** and **convert** Chef recipes or Puppet manifests into clean, production-ready **Ansible Playbooks**,  
powered by **LlamaStack Agents** and **Retrieval-Augmented Generation (RAG)**.

This project combines a **local LlamaStack server**, **Ollama model serving**, and a **Streamlit UI** to streamline Infrastructure-as-Code modernization.

---



## ğŸš€ What This Solution Does

- **Chef/Puppet â†’ Ansible Playbook Conversion:**
  - Upload, browse, or Git-clone Chef/Puppet code.
  - An agent analyzes the code, retrieves best practices via RAG, and generates clean Ansible Playbooks.

- **Code Analysis (Explain Mode):**
  - Alternatively, you can ask the agent to **analyze** your code â€”  
    it explains what the Chef/Puppet scripts are doing, in professional plain English.

- **Streaming UI:**
  - Results stream live into the Streamlit interface: side-by-side Analysis and Playbook views.

- **Built-in RAG Retrieval:**
  - The agent uses a custom `ansible_rules` vector database to fetch Ansible best practices during playbook generation.

- **Agentic Execution:**
  - Everything is done by LlamaStack Agents â€” no direct LLM calls â€” making it modular, explainable, and extendable.

---

## Solution Design 
Start
  |
  v
+---------------------------+
| 1ï¸âƒ£ Load Puppet/Chef Code |
+---------------------------+
  |
  v
+------------------------------------+
| 2ï¸âƒ£ Create Agent A (RAG Agent)     |
|  â€¢ Tool: builtin::rag/knowledge_search |
+------------------------------------+
  |
  v
+------------------------+
| ğŸ” Turn 1 (Agent A)    |
| Input: "puppet file resource"      |
| Output: RAG context chunks         |
+------------------------+
  |
  v
+---------------------------------------+
| 3ï¸âƒ£ Combine:                          |
|  â€¢ Puppet Code                        |
|  â€¢ Retrieved RAG Context              |
|  â†’ Build single combined LLM prompt   |
+---------------------------------------+
  |
  v
+------------------------------+
| 4ï¸âƒ£ Create Agent B (LLM Agent) |
|  â€¢ No tools                   |
|  â€¢ Instructions depend on mode|
+------------------------------+
  |
  v
+------------------------+
| ğŸ” Turn 2 (Agent B)    |
| Input: Combined prompt |
| Output:                |
|  â€¢ If mode == analyze: English summary |
|  â€¢ If mode == convert: Ansible YAML    |
+------------------------+
  |
  v
Done ğŸ‰


# âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/rrbanda/x2ansible.git
cd x2ansible

```

---

### 2. Set Up Python Environment

**Create and activate a virtual environment:**

```bash
python3 -m venv .venv
source .venv/bin/activate
```

---

### 3. Install Required Python Packages

**Install dependencies from `requirements.txt`:**

```bash
pip install -r requirements.txt
```

---

### 4. Start Ollama and Load Granite Model

**In a new terminal:**

```bash
ollama run granite-code:8b
```

- This starts the `granite-code:8b` model locally for inference.

---

### 5. Build and Run the LlamaStack Server

**In another new terminal:**

```bash
INFERENCE_MODEL=granite-code:8b uv run --with llama-stack llama stack build --template ollama --image-type venv --run
```

- This builds and starts your local LlamaStack server, connected to your Ollama model.

---

### 6. Configure and Register Your Model

**In your project virtual environment:**

- Configure the LlamaStack client:

```bash
llama-stack-client configure --endpoint http://localhost:8321 --api-key none
```

- Register your granite model:

```bash
llama-stack-client models register granite-code:8b
```

- Confirm that the model is registered:

```bash
llama-stack-client models list
```

You should see `granite-code:8b` and `llama3.2:3b` listed.

---

### 7. Launch the Streamlit App

**Still in your project root with the virtual environment activated:**

```bash
streamlit run app.py
```

The Streamlit UI will open at [http://localhost:8501](http://localhost:8501).

---

## ğŸ“š How to Use the App

| Step | Action |
|:---|:---|
| 1 | Select **Agentic** backend in the sidebar. |
| 2 | Choose your **file source**: Upload, Browse local files, or Git Clone. |
| 3 | Upload or select Chef/Puppet files for analysis or conversion. |
| 4 | Click **ğŸš€ Start Conversion**. |
| 5 | Watch live **Analysis** and **Ansible Playbook** generation side-by-side! |
| 6 | Download final playbooks and summaries from the interface if needed. |

---

## ğŸ§  Internal Agent Architecture

```plaintext
User Input (Chef/Puppet Code)
          â†“
AgenticModel.transform(mode="analyze" or "convert")
          â†“
Creates LlamaStack Agent dynamically
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ builtin::rag Tool (Vector DB: ansible_rules) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“ (retrieved best practices)
Agent enriches context
          â†“
LLM Inference (via LlamaStack Model: llama3.2:3b)
          â†“
Streaming Output
          â†“
Frontend Display (Streamlit)
          â†“
Optional Post-processing (sanitize YAML, flatten blocks)
          â†“
Final Analysis / Playbook Output
```

---

# ğŸ“¦ Project Structure

```plaintext
â”œâ”€â”€ app.py                    # Streamlit UI application
â”œâ”€â”€ ai_modules/
â”‚   â””â”€â”€ agentic_model.py       # Agent wrapper handling inference
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ analyze_instructions.txt  # Text prompt for analysis mode
â”‚   â””â”€â”€ convert_instructions.txt  # Text prompt for convert mode
â”œâ”€â”€ uploads/                  # Uploaded input files
â”œâ”€â”€ results/                   # Generated playbooks and summaries
â”œâ”€â”€ settings.config            # Settings (e.g., default folders)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ app.log                 # Logging output
â””â”€â”€ README.md                  # This file!
```

---

# ğŸ›¡ï¸ Key Features

- True agentic execution (no direct LLM prompt hacking).
- Best-practice Ansible playbook generation via RAG enrichment.
- Automatic creation of `ansible_rules` vector database.
- Externalized instruction prompts (easy to customize).
- Full logging (`logs/app.log`).
- Streaming live output for smooth UI experience.

---

# ğŸ“¢ Notes

- Playbooks generated assume a **RHEL-based** system (preferring `yum` over `apt`).
- `when: true` conditions are automatically removed if unnecessary.
- `.erb` templates should be replaced by `.j2` templates manually after conversion (or post-processing improvements can be added).

---

#  Contributing

We welcome PRs, issues, suggestions, and ideas to improve this project!

---

# ğŸ“œ License

MIT License

---
